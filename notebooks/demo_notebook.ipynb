{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Local RAG Pipeline - Complete Demonstration\n",
    "\n",
    "This notebook demonstrates all features of the Local RAG Pipeline, including:\n",
    "- Document loading and processing\n",
    "- Vector index creation\n",
    "- Knowledge graph building\n",
    "- Different search modes (vector, graph, hybrid)\n",
    "- Question answering\n",
    "- Visualization and analysis\n",
    "\n",
    "**Time to complete:** ~15-20 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup and Installation\n",
    "\n",
    "First, let's make sure we have all dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install sentence-transformers faiss-cpu networkx transformers langchain langchain-community\n",
    "# !pip install torch numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Import the RAG pipeline\n",
    "from rag_pipeline import LocalRAGPipeline, Document\n",
    "\n",
    "# Import utilities for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Part 1: Creating Sample Documents\n",
    "\n",
    "Let's create some sample documents to demonstrate the system. We'll create documents about AI, machine learning, and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 5 sample documents in demo_documents\n",
      "\n",
      "Documents created:\n",
      "  üìÑ ai_intro.txt\n",
      "  üìÑ data_science.txt\n",
      "  üìÑ deep_learning.txt\n",
      "  üìÑ machine_learning.txt\n",
      "  üìÑ nlp_transformers.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a sample documents directory\n",
    "docs_dir = Path(\"demo_documents\")\n",
    "docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Document 1: AI Introduction\n",
    "(docs_dir / \"ai_intro.txt\").write_text(\"\"\"\n",
    "Artificial Intelligence (AI) Overview\n",
    "\n",
    "Artificial Intelligence is the simulation of human intelligence processes by machines, \n",
    "especially computer systems. These processes include learning, reasoning, and self-correction.\n",
    "\n",
    "Key AI Concepts:\n",
    "- Machine Learning: Systems that learn from data\n",
    "- Deep Learning: Neural networks with multiple layers\n",
    "- Natural Language Processing: Understanding human language\n",
    "- Computer Vision: Understanding visual information\n",
    "\n",
    "AI applications are transforming industries including healthcare, finance, transportation, \n",
    "and education. The field combines computer science, mathematics, and cognitive science.\n",
    "\"\"\")\n",
    "\n",
    "# Document 2: Machine Learning\n",
    "(docs_dir / \"machine_learning.txt\").write_text(\"\"\"\n",
    "Machine Learning Fundamentals\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn \n",
    "and improve from experience without being explicitly programmed. It focuses on developing \n",
    "computer programs that can access data and learn from it.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning from labeled data\n",
    "   - Classification: Categorizing data into classes\n",
    "   - Regression: Predicting continuous values\n",
    "\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "   - Clustering: Grouping similar data points\n",
    "   - Dimensionality Reduction: Reducing feature space\n",
    "\n",
    "3. Reinforcement Learning: Learning through interaction and feedback\n",
    "\n",
    "Popular algorithms include decision trees, random forests, support vector machines, \n",
    "and neural networks. Python libraries like scikit-learn and TensorFlow are widely used.\n",
    "\"\"\")\n",
    "\n",
    "# Document 3: Deep Learning\n",
    "(docs_dir / \"deep_learning.txt\").write_text(\"\"\"\n",
    "Deep Learning and Neural Networks\n",
    "\n",
    "Deep learning is a specialized subset of machine learning that uses neural networks \n",
    "with multiple layers (deep neural networks) to progressively extract higher-level \n",
    "features from raw input.\n",
    "\n",
    "Neural Network Architecture:\n",
    "- Input Layer: Receives the raw data\n",
    "- Hidden Layers: Process and transform the data\n",
    "- Output Layer: Produces the final prediction\n",
    "\n",
    "Common Deep Learning Architectures:\n",
    "- Convolutional Neural Networks (CNN): Excellent for image processing\n",
    "- Recurrent Neural Networks (RNN): Great for sequential data\n",
    "- Transformers: Revolutionary architecture for NLP tasks\n",
    "- Generative Adversarial Networks (GAN): For generating new data\n",
    "\n",
    "Deep learning has achieved breakthrough results in image recognition, natural language \n",
    "processing, speech recognition, and game playing (like AlphaGo).\n",
    "\"\"\")\n",
    "\n",
    "# Document 4: Data Science\n",
    "(docs_dir / \"data_science.txt\").write_text(\"\"\"\n",
    "Data Science: Extracting Insights from Data\n",
    "\n",
    "Data science is an interdisciplinary field that uses scientific methods, processes, \n",
    "algorithms, and systems to extract knowledge and insights from structured and \n",
    "unstructured data.\n",
    "\n",
    "Data Science Workflow:\n",
    "1. Data Collection: Gathering relevant data from various sources\n",
    "2. Data Cleaning: Handling missing values, outliers, and inconsistencies\n",
    "3. Exploratory Data Analysis: Understanding patterns and relationships\n",
    "4. Feature Engineering: Creating relevant features for modeling\n",
    "5. Model Building: Developing predictive or descriptive models\n",
    "6. Model Evaluation: Assessing model performance\n",
    "7. Deployment: Putting models into production\n",
    "\n",
    "Essential Tools:\n",
    "- Python: Primary programming language with pandas, NumPy, matplotlib\n",
    "- R: Statistical computing and graphics\n",
    "- SQL: Database querying\n",
    "- Jupyter: Interactive development environment\n",
    "\n",
    "Data scientists combine domain knowledge, programming skills, and statistical expertise.\n",
    "\"\"\")\n",
    "\n",
    "# Document 5: NLP and Transformers\n",
    "(docs_dir / \"nlp_transformers.txt\").write_text(\"\"\"\n",
    "Natural Language Processing and Transformers\n",
    "\n",
    "Natural Language Processing (NLP) enables computers to understand, interpret, and \n",
    "generate human language. It combines computational linguistics with machine learning.\n",
    "\n",
    "Key NLP Tasks:\n",
    "- Text Classification: Categorizing text into predefined classes\n",
    "- Named Entity Recognition: Identifying entities like names, locations\n",
    "- Sentiment Analysis: Determining emotional tone\n",
    "- Machine Translation: Translating between languages\n",
    "- Question Answering: Providing answers to natural language questions\n",
    "- Text Summarization: Creating concise summaries\n",
    "\n",
    "The Transformer Revolution:\n",
    "Transformers, introduced in 2017, use self-attention mechanisms to process sequences \n",
    "in parallel rather than sequentially. This architecture powers models like:\n",
    "- BERT: Bidirectional Encoder Representations from Transformers\n",
    "- GPT: Generative Pre-trained Transformer\n",
    "- T5: Text-to-Text Transfer Transformer\n",
    "\n",
    "These models have achieved state-of-the-art results across numerous NLP benchmarks.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Created {len(list(docs_dir.glob('*.txt')))} sample documents in {docs_dir}\")\n",
    "print(\"\\nDocuments created:\")\n",
    "for doc in sorted(docs_dir.glob('*.txt')):\n",
    "    print(f\"  üìÑ {doc.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Part 2: Initialize the RAG Pipeline\n",
    "\n",
    "Now let's initialize our RAG pipeline with vector search and knowledge graph capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RAG Pipeline...\n",
      "\n",
      "‚è≥ This may take a few minutes on first run (downloading models)...\n",
      "\n",
      "Initializing RAG Pipeline...\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Loading LLM: microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Initializing RAG Pipeline...\\n\")\n",
    "print(\"‚è≥ This may take a few minutes on first run (downloading models)...\\n\")\n",
    "\n",
    "# Initialize with lightweight models for faster demo\n",
    "rag = LocalRAGPipeline(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",  # Fast, lightweight (~80MB)\n",
    "    llm_model=\"microsoft/phi-2\",          # Small but capable (~5GB)\n",
    "    chunk_size=300,                       # Smaller chunks for demo\n",
    "    chunk_overlap=50,\n",
    "    storage_path=\"./demo_rag_storage\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ RAG Pipeline initialized successfully!\")\n",
    "print(f\"   - Embedding dimension: {rag.embedding_dim}\")\n",
    "print(f\"   - Storage path: {rag.storage_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Part 3: Load and Process Documents\n",
    "\n",
    "Let's load our sample documents and split them into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Loading documents...\\n\")\n",
    "\n",
    "documents = rag.load_documents(str(docs_dir))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} document chunks\\n\")\n",
    "print(\"Sample chunks:\")\n",
    "print(\"=\" * 80)\n",
    "for i, doc in enumerate(documents[:3], 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content preview: {doc.content[:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Part 4: Build Vector Index\n",
    "\n",
    "Create embeddings and build the FAISS vector index for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Building vector index...\\n\")\n",
    "\n",
    "rag.build_vector_index(documents)\n",
    "\n",
    "print(f\"\\n‚úÖ Vector index built successfully!\")\n",
    "print(f\"   - Total vectors: {len(documents)}\")\n",
    "print(f\"   - Vector dimension: {rag.embedding_dim}\")\n",
    "print(f\"   - Index type: FAISS FlatL2\")\n",
    "\n",
    "# Show embedding statistics\n",
    "if documents and documents[0].embedding is not None:\n",
    "    sample_embedding = documents[0].embedding\n",
    "    print(f\"\\nüìä Embedding statistics:\")\n",
    "    print(f\"   - Mean: {sample_embedding.mean():.4f}\")\n",
    "    print(f\"   - Std: {sample_embedding.std():.4f}\")\n",
    "    print(f\"   - Min: {sample_embedding.min():.4f}\")\n",
    "    print(f\"   - Max: {sample_embedding.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üï∏Ô∏è Part 5: Build Knowledge Graph\n",
    "\n",
    "Extract entities and build a knowledge graph showing relationships between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üï∏Ô∏è  Building knowledge graph...\\n\")\n",
    "\n",
    "rag.build_knowledge_graph(documents)\n",
    "\n",
    "print(f\"\\n‚úÖ Knowledge graph built successfully!\")\n",
    "print(f\"   - Nodes: {rag.knowledge_graph.number_of_nodes()}\")\n",
    "print(f\"   - Edges: {rag.knowledge_graph.number_of_edges()}\")\n",
    "\n",
    "# Analyze the graph\n",
    "node_types = {}\n",
    "for node, data in rag.knowledge_graph.nodes(data=True):\n",
    "    node_type = data.get('type', 'unknown')\n",
    "    node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "\n",
    "print(f\"\\nüìä Graph composition:\")\n",
    "for node_type, count in node_types.items():\n",
    "    print(f\"   - {node_type.capitalize()}: {count}\")\n",
    "\n",
    "# Show most connected entities\n",
    "degrees = dict(rag.knowledge_graph.degree())\n",
    "top_entities = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(f\"\\nüîó Most connected entities:\")\n",
    "for entity, degree in top_entities:\n",
    "    node_type = rag.knowledge_graph.nodes[entity].get('type', 'unknown')\n",
    "    print(f\"   - {entity[:30]:30s} ({node_type}): {degree} connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Part 6: Visualize the Knowledge Graph\n",
    "\n",
    "Let's visualize the relationships in our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subgraph of the most connected nodes for visualization\n",
    "degrees = dict(rag.knowledge_graph.degree())\n",
    "top_nodes = sorted(degrees.keys(), key=lambda x: degrees[x], reverse=True)[:20]\n",
    "subgraph = rag.knowledge_graph.subgraph(top_nodes)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Create layout\n",
    "pos = nx.spring_layout(subgraph, k=3, iterations=50, seed=42)\n",
    "\n",
    "# Color nodes by type\n",
    "node_colors = []\n",
    "for node in subgraph.nodes():\n",
    "    node_type = subgraph.nodes[node].get('type', 'unknown')\n",
    "    if node_type == 'document':\n",
    "        node_colors.append('lightblue')\n",
    "    elif node_type == 'entity':\n",
    "        node_colors.append('lightgreen')\n",
    "    else:\n",
    "        node_colors.append('lightgray')\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw_networkx_nodes(\n",
    "    subgraph, pos,\n",
    "    node_color=node_colors,\n",
    "    node_size=1000,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph, pos,\n",
    "    edge_color='gray',\n",
    "    arrows=True,\n",
    "    arrowsize=15,\n",
    "    alpha=0.5,\n",
    "    width=2\n",
    ")\n",
    "\n",
    "# Draw labels\n",
    "labels = {node: node[:20] + '...' if len(node) > 20 else node for node in subgraph.nodes()}\n",
    "nx.draw_networkx_labels(\n",
    "    subgraph, pos,\n",
    "    labels,\n",
    "    font_size=9,\n",
    "    font_weight='bold'\n",
    ")\n",
    "\n",
    "plt.title(\"Knowledge Graph - Top 20 Connected Nodes\\n(Blue=Documents, Green=Entities)\", \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Graph Statistics:\")\n",
    "print(f\"   - Average degree: {sum(degrees.values()) / len(degrees):.2f}\")\n",
    "print(f\"   - Graph density: {nx.density(rag.knowledge_graph):.4f}\")\n",
    "if nx.is_connected(subgraph.to_undirected()):\n",
    "    print(f\"   - Average path length: {nx.average_shortest_path_length(subgraph.to_undirected()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîé Part 7: Vector Search Demo\n",
    "\n",
    "Let's try vector-based semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is deep learning and how does it work?\"\n",
    "\n",
    "print(f\"üîç Vector Search Query: '{query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vector_results = rag.vector_search(query, top_k=3)\n",
    "\n",
    "print(f\"\\nüìö Top {len(vector_results)} Results:\\n\")\n",
    "\n",
    "for i, (doc, score) in enumerate(vector_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  üìÑ Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"  üìä Similarity Score: {1/(1+score):.4f} (distance: {score:.4f})\")\n",
    "    print(f\"  üìù Content: {doc.content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üï∏Ô∏è Part 8: Graph Search Demo\n",
    "\n",
    "Now let's try graph-based search using relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about neural networks and transformers\"\n",
    "\n",
    "print(f\"üï∏Ô∏è  Graph Search Query: '{query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "graph_results = rag.graph_search(query, top_k=3)\n",
    "\n",
    "print(f\"\\nüìö Top {len(graph_results)} Results:\\n\")\n",
    "\n",
    "for i, doc in enumerate(graph_results, 1):\n",
    "    # Get graph metrics for this document\n",
    "    degree = rag.knowledge_graph.degree(doc.doc_id)\n",
    "    \n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  üìÑ Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"  üîó Graph Connections: {degree}\")\n",
    "    print(f\"  üìù Content: {doc.content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öñÔ∏è Part 9: Hybrid Search Demo\n",
    "\n",
    "Combine both vector and graph search for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the applications of machine learning?\"\n",
    "\n",
    "print(f\"‚öñÔ∏è  Hybrid Search Query: '{query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try different weight combinations\n",
    "weight_combos = [\n",
    "    (0.8, 0.2, \"Vector-focused\"),\n",
    "    (0.7, 0.3, \"Balanced (default)\"),\n",
    "    (0.3, 0.7, \"Graph-focused\")\n",
    "]\n",
    "\n",
    "for vector_w, graph_w, label in weight_combos:\n",
    "    print(f\"\\nüéØ {label} (Vector: {vector_w}, Graph: {graph_w})\\n\")\n",
    "    \n",
    "    hybrid_results = rag.hybrid_search(\n",
    "        query, \n",
    "        top_k=3, \n",
    "        vector_weight=vector_w,\n",
    "        graph_weight=graph_w\n",
    "    )\n",
    "    \n",
    "    for i, doc in enumerate(hybrid_results, 1):\n",
    "        print(f\"  {i}. {doc.metadata.get('source', 'Unknown')}: {doc.content[:100]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ Part 10: Question Answering\n",
    "\n",
    "Now let's use the complete RAG pipeline to answer questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning\",\n",
    "    \"What are transformers in NLP?\"\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Question Answering with RAG Pipeline\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = rag.query(question, search_type=\"hybrid\", top_k=3)\n",
    "    \n",
    "    print(f\"\\nü§ñ Answer:\\n\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    print(f\"\\n\\nüìö Sources:\")\n",
    "    for j, doc in enumerate(result['retrieved_documents'][:3], 1):\n",
    "        print(f\"\\n  {j}. {doc['metadata']['source']}\")\n",
    "        print(f\"     {doc['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Part 11: Performance Analysis\n",
    "\n",
    "Let's analyze search performance and compare different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_queries = [\n",
    "    \"machine learning algorithms\",\n",
    "    \"neural network architecture\",\n",
    "    \"data science workflow\",\n",
    "    \"natural language processing\",\n",
    "    \"deep learning applications\"\n",
    "]\n",
    "\n",
    "print(\"‚è±Ô∏è  Performance Comparison\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {'vector': [], 'graph': [], 'hybrid': []}\n",
    "\n",
    "for query in test_queries:\n",
    "    # Vector search\n",
    "    start = time.time()\n",
    "    rag.vector_search(query, top_k=5)\n",
    "    results['vector'].append(time.time() - start)\n",
    "    \n",
    "    # Graph search\n",
    "    start = time.time()\n",
    "    rag.graph_search(query, top_k=5)\n",
    "    results['graph'].append(time.time() - start)\n",
    "    \n",
    "    # Hybrid search\n",
    "    start = time.time()\n",
    "    rag.hybrid_search(query, top_k=5)\n",
    "    results['hybrid'].append(time.time() - start)\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\nüìä Average Query Time (seconds):\\n\")\n",
    "for method, times in results.items():\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"  {method.capitalize():10s}: {avg_time:.4f}s\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "methods = list(results.keys())\n",
    "avg_times = [sum(results[m])/len(results[m]) for m in methods]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = plt.bar(methods, avg_times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time in zip(bars, avg_times):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{time:.4f}s',\n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.xlabel('Search Method', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Average Time (seconds)', fontsize=14, fontweight='bold')\n",
    "plt.title('Search Performance Comparison', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Part 12: Save and Load Pipeline\n",
    "\n",
    "Demonstrate persistence - save the pipeline and reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving pipeline...\\n\")\n",
    "\n",
    "rag.save()\n",
    "\n",
    "print(f\"‚úÖ Pipeline saved to: {rag.storage_path}\\n\")\n",
    "print(\"Files saved:\")\n",
    "for file in rag.storage_path.glob('*'):\n",
    "    size = file.stat().st_size / 1024  # KB\n",
    "    print(f\"  üìÑ {file.name:30s} ({size:>8.1f} KB)\")\n",
    "\n",
    "# Now reload it\n",
    "print(\"\\n\\nüì¶ Loading pipeline from disk...\\n\")\n",
    "\n",
    "rag_reloaded = LocalRAGPipeline(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"microsoft/phi-2\",\n",
    "    storage_path=\"./demo_rag_storage\"\n",
    ")\n",
    "\n",
    "rag_reloaded.load()\n",
    "\n",
    "print(\"‚úÖ Pipeline reloaded successfully!\\n\")\n",
    "print(f\"   - Documents: {len(rag_reloaded.documents)}\")\n",
    "print(f\"   - Graph nodes: {rag_reloaded.knowledge_graph.number_of_nodes()}\")\n",
    "print(f\"   - Graph edges: {rag_reloaded.knowledge_graph.number_of_edges()}\")\n",
    "\n",
    "# Test the reloaded pipeline\n",
    "print(\"\\n\\nüß™ Testing reloaded pipeline...\\n\")\n",
    "\n",
    "test_result = rag_reloaded.query(\n",
    "    \"What is machine learning?\",\n",
    "    search_type=\"hybrid\",\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(f\"Answer: {test_result['answer'][:200]}...\")\n",
    "print(\"\\n‚úÖ Reloaded pipeline works perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Part 13: Advanced Analysis\n",
    "\n",
    "Let's perform some advanced analysis on our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Advanced Knowledge Base Analysis\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Document similarity matrix\n",
    "print(\"\\n1Ô∏è‚É£  Document Similarity Analysis\\n\")\n",
    "\n",
    "doc_embeddings = [doc.embedding for doc in documents if doc.embedding is not None]\n",
    "if doc_embeddings:\n",
    "    # Calculate pairwise similarities\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(doc_embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.title('Document Chunk Similarity Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Document Chunk Index')\n",
    "    plt.ylabel('Document Chunk Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   Average inter-document similarity: {similarity_matrix.mean():.4f}\")\n",
    "\n",
    "# 2. Entity frequency analysis\n",
    "print(\"\\n\\n2Ô∏è‚É£  Entity Frequency Analysis\\n\")\n",
    "\n",
    "entity_counts = {}\n",
    "for node, data in rag.knowledge_graph.nodes(data=True):\n",
    "    if data.get('type') == 'entity':\n",
    "        entity_counts[node] = rag.knowledge_graph.degree(node)\n",
    "\n",
    "top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "entities, counts = zip(*top_entities) if top_entities else ([], [])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(entities)), counts, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.yticks(range(len(entities)), [e[:30] for e in entities])\n",
    "plt.xlabel('Number of Connections', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Entity', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 10 Most Connected Entities', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Search relevance comparison\n",
    "print(\"\\n\\n3Ô∏è‚É£  Search Method Relevance Comparison\\n\")\n",
    "\n",
    "test_query = \"deep learning neural networks\"\n",
    "\n",
    "vector_docs = [doc for doc, _ in rag.vector_search(test_query, top_k=3)]\n",
    "graph_docs = rag.graph_search(test_query, top_k=3)\n",
    "hybrid_docs = rag.hybrid_search(test_query, top_k=3)\n",
    "\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "print(\"Vector Search Results:\")\n",
    "for i, doc in enumerate(vector_docs, 1):\n",
    "    print(f\"  {i}. {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "print(\"\\nGraph Search Results:\")\n",
    "for i, doc in enumerate(graph_docs, 1):\n",
    "    print(f\"  {i}. {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "print(\"\\nHybrid Search Results:\")\n",
    "for i, doc in enumerate(hybrid_docs, 1):\n",
    "    print(f\"  {i}. {doc.metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Part 14: Interactive Q&A Session\n",
    "\n",
    "Now you can ask your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A cell - run this multiple times with different questions!\n",
    "\n",
    "your_question = \"What are the main types of machine learning?\"  # ‚Üê Change this!\n",
    "\n",
    "print(f\"‚ùì Your Question: {your_question}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüîç Searching knowledge base...\\n\")\n",
    "\n",
    "result = rag.query(your_question, search_type=\"hybrid\", top_k=5)\n",
    "\n",
    "print(\"ü§ñ Answer:\\n\")\n",
    "display(Markdown(result['answer']))\n",
    "\n",
    "print(\"\\n\\nüìö Retrieved Sources:\\n\")\n",
    "for i, doc in enumerate(result['retrieved_documents'][:3], 1):\n",
    "    print(f\"{i}. **{doc['metadata']['source']}**\")\n",
    "    print(f\"   _{doc['content'][:200]}_...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Part 15: System Statistics and Summary\n",
    "\n",
    "Let's review what we've built and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä RAG PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìö Document Statistics:\")\n",
    "print(f\"   - Total source documents: {len(list(docs_dir.glob('*.txt')))}\")\n",
    "print(f\"   - Total chunks: {len(rag.documents)}\")\n",
    "print(f\"   - Average chunk size: {sum(len(d.content) for d in rag.documents) / len(rag.documents):.0f} chars\")\n",
    "\n",
    "print(\"\\nüîç Vector Index:\")\n",
    "print(f\"   - Embedding model: {rag.embedding_model.__class__.__name__}\")\n",
    "print(f\"   - Vector dimension: {rag.embedding_dim}\")\n",
    "print(f\"   - Index type: FAISS FlatL2\")\n",
    "print(f\"   - Total vectors: {len(rag.documents)}\")\n",
    "\n",
    "print(\"\\nüï∏Ô∏è  Knowledge Graph:\")\n",
    "print(f\"   - Total nodes: {rag.knowledge_graph.number_of_nodes()}\")\n",
    "print(f\"   - Total edges: {rag.knowledge_graph.number_of_edges()}\")\n",
    "print(f\"   - Graph density: {nx.density(rag.knowledge_graph):.4f}\")\n",
    "degrees = dict(rag.knowledge_graph.degree())\n",
    "print(f\"   - Average degree: {sum(degrees.values()) / len(degrees):.2f}\")\n",
    "\n",
    "print(\"\\nü§ñ Generation:\")\n",
    "print(f\"   - LLM: {rag.llm.config.name_or_path}\")\n",
    "print(f\"   - Max tokens: 200 (default)\")\n",
    "print(f\"   - Temperature: 0.7\")\n",
    "\n",
    "print(\"\\nüíæ Storage:\")\n",
    "print(f\"   - Storage path: {rag.storage_path}\")\n",
    "total_size = sum(f.stat().st_size for f in rag.storage_path.glob('*'))\n",
    "print(f\"   - Total size: {total_size / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline Features:\")\n",
    "features = [\n",
    "    \"‚úì Vector similarity search (FAISS)\",\n",
    "    \"‚úì Knowledge graph relationships (NetworkX)\",\n",
    "    \"‚úì Hybrid search with configurable weights\",\n",
    "    \"‚úì Local LLM generation (no external APIs)\",\n",
    "    \"‚úì Persistent storage (save/load)\",\n",
    "    \"‚úì Multi-format document support\",\n",
    "    \"‚úì Source attribution\",\n",
    "    \"‚úì Configurable chunk sizes\"\n",
    "]\n",
    "for feature in features:\n",
    "    print(f\"   {feature}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Demo Complete! You've successfully explored the Local RAG Pipeline.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ Part 16: Cleanup (Optional)\n",
    "\n",
    "Uncomment and run this cell to clean up demo files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# print(\"üßπ Cleaning up demo files...\\n\")\n",
    "\n",
    "# # Remove demo documents\n",
    "# if docs_dir.exists():\n",
    "#     shutil.rmtree(docs_dir)\n",
    "#     print(f\"‚úÖ Removed {docs_dir}\")\n",
    "\n",
    "# # Remove demo storage\n",
    "# if rag.storage_path.exists():\n",
    "#     shutil.rmtree(rag.storage_path)\n",
    "#     print(f\"‚úÖ Removed {rag.storage_path}\")\n",
    "\n",
    "# print(\"\\n‚úÖ Cleanup complete!\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Uncomment the code above to clean up demo files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Next Steps\n",
    "\n",
    "Now that you've explored the RAG pipeline, try:\n",
    "\n",
    "1. **Use Your Own Documents**: Replace the demo documents with your PDFs, DOCX files, etc.\n",
    "2. **Experiment with Models**: Try different embedding models or LLMs\n",
    "3. **Tune Parameters**: Adjust chunk sizes, search weights, top_k values\n",
    "4. **Extend Functionality**: Add custom entity extraction, reranking, or filters\n",
    "5. **Build Applications**: Create a web UI, API server, or chatbot\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- **Documentation**: Check the `docs/` folder for complete guides\n",
    "- **Examples**: See `examples/example_usage.py` for more code samples\n",
    "- **Architecture**: Read `docs/ARCHITECTURE.md` for technical details\n",
    "- **API Reference**: Full API documentation available\n",
    "\n",
    "### ü§ù Contributing\n",
    "\n",
    "Found a bug or have a feature idea? Contributions are welcome!\n",
    "- GitHub: [repository link]\n",
    "- Issues: [issues link]\n",
    "- Discussions: [discussions link]\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Thank you for exploring the Local RAG Pipeline!**\n",
    "\n",
    "Built with ‚ù§Ô∏è for privacy-conscious developers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
